{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "def derive_sentiment(sent_file, tweet_file):\n",
    "    #Create sentiment dictionary    \n",
    "    sent_dict = dict()\n",
    "    for line in sent_file:\n",
    "        split_line = line.split()\n",
    "        if len(split_line) > 2:\n",
    "            continue\n",
    "        sent_dict[split_line[0]] = int(split_line[1])\n",
    "\n",
    "    for line in tweet_file:\n",
    "        test = json.loads(line)\n",
    "        count = 0\n",
    "        if 'text' in test:\n",
    "            #print(test['text'])\n",
    "            text1 = test['text'].split()\n",
    "            for word in text1:\n",
    "                #print(word)\n",
    "                if word in sent_dict:\n",
    "                    #print(word + '!!!!')\n",
    "                    count += sent_dict[word]\n",
    "        print(count)\n",
    "\n",
    "sent_file = open('./SENTIMENT.txt')\n",
    "tweet_file = open('./3minutes.json')\n",
    "#derive_sentiment(sent_file, tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_terms_sentiments(sent_file, tweet_file):\n",
    "    #Create sentiment dictionary    \n",
    "    sent_dict = dict()\n",
    "    for line in sent_file:\n",
    "        split_line = line.split()\n",
    "        if len(split_line) > 2:\n",
    "            continue\n",
    "        sent_dict[split_line[0]] = int(split_line[1])\n",
    "\n",
    "    #Compute score for each tweet\n",
    "    non_sent_dict = dict()\n",
    "    #i = 0\n",
    "    for line in tweet_file:\n",
    "        test = json.loads(line)\n",
    "        count = 0\n",
    "        if 'text' in test:\n",
    "            #print(test['text'])\n",
    "            text1 = test['text'].split()\n",
    "            for word in text1:\n",
    "                #print(word)\n",
    "                if word in sent_dict:\n",
    "                    #print(word + '!!!!')\n",
    "                    count += sent_dict[word]\n",
    "            for word in text1:\n",
    "                if (word not in sent_dict):\n",
    "                    if word not in non_sent_dict:\n",
    "                        #print('Adding word: ', word, ' with count: ', count)\n",
    "                        non_sent_dict[word] = (count,1)\n",
    "                    else: \n",
    "                        non_sent_dict[word] = (count,non_sent_dict[word][1]+1)\n",
    "    for w in non_sent_dict:\n",
    "        count_ratio = non_sent_dict[w][0]/non_sent_dict[w][1]\n",
    "        print(w + ' ', count_ratio)\n",
    "        \n",
    "#new_terms_sentiments(sent_file, tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(tweet_file):\n",
    "    word_dict = dict()\n",
    "    count = 0\n",
    "    for line in tweet_file:\n",
    "        test = json.loads(line)\n",
    "        if 'text' in test:\n",
    "            text1 = test['text'].split()\n",
    "            for word in text1:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word] += 1\n",
    "                else:\n",
    "                    word_dict[word] = 1\n",
    "                count += 1            \n",
    "\n",
    "    for key in word_dict:\n",
    "        #word_dict[key] = word_dict[key]/count\n",
    "        print(key + ' ' + str(word_dict[key]/count))\n",
    "\n",
    "#term_frequency(tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "def happiest_state(sent_file, tweet_file):\n",
    "    #Create sentiment dictionary    \n",
    "    sent_dict = dict()\n",
    "    for line in sent_file:\n",
    "        split_line = line.split()\n",
    "        if len(split_line) > 2:\n",
    "            continue\n",
    "        sent_dict[split_line[0]] = int(split_line[1])\n",
    "\n",
    "    for line in tweet_file:\n",
    "        test = json.loads(line)\n",
    "        count = 0\n",
    "        if 'text' in test:\n",
    "            text1 = test['text'].split()\n",
    "            for word in text1:\n",
    "                if word in sent_dict:\n",
    "                    count += sent_dict[word]\n",
    "        #print(count)\n",
    "        #Find the state\n",
    "        #If found, add count\n",
    "        state_happ = dict()\n",
    "        for state in states: \n",
    "            state_happ[state] = [0,0]\n",
    "        if 'user' in test:\n",
    "            text2 = test['user']['location'].split()\n",
    "            for state in states:\n",
    "                for word in text2:\n",
    "                    if (state in word) or (states[state] in word):\n",
    "                        state_happ[state][0] = state_happ[state][0] + count\n",
    "                        state_happ[state][1] = state_happ[state][1] + 1\n",
    "    avg_happ = dict()\n",
    "    for state in state_happ:\n",
    "        avg_happ[state] = state_happ[state][0]/(state_happ[state][1]+1)\n",
    "    max_happ = ['AK', avg_happ['AK']]\n",
    "    for state in avg_happ:\n",
    "        if max_happ[1] < avg_happ[state]:\n",
    "            max_happ[0] = state\n",
    "            max_happ[1] = avg_happ[state]\n",
    "\n",
    "    print(max_happ[0],max_happ[1])\n",
    "    \n",
    "#happiest_state(sent_file, tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ten_hashtags(tweet_file):\n",
    "    word_dict = dict()\n",
    "    for line in tweet_file:\n",
    "        test = json.loads(line)\n",
    "        print(test)\n",
    "        if 'text' in test:\n",
    "            text1 = test['text'].split()\n",
    "            for word in text1:\n",
    "                if '#' in word:\n",
    "                    if word in word_dict:\n",
    "                        word_dict[word] += 1\n",
    "                    else:\n",
    "                        word_dict[word] = 1\n",
    "\n",
    "    list_values = list(word_dict.values())\n",
    "    list_values.sort(reverse=True)\n",
    "    top_ten_values = list_values[:10]\n",
    "    top_ten_dict = dict()\n",
    "    i = 0\n",
    "    for key in word_dict:\n",
    "        if word_dict[key] in top_ten_values:\n",
    "            top_ten_dict[key] = word_dict[key]\n",
    "            top_ten_values.remove(word_dict[key])\n",
    "            i += 1\n",
    "        #if i == 10:\n",
    "        #    break\n",
    "        if len(top_ten_dict) == 10:\n",
    "            break\n",
    "    for e in top_ten_dict:\n",
    "        print(e[1:] + ' ' + str(top_ten_dict[e]))\n",
    "\n",
    "#top_ten_hashtags(tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
